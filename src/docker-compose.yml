services:
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
    volumes:
      - qdrant_storage:/qdrant/storage

  # Context Retrieval Service
  context-retrieval:
    build:
      context: ./context-retrieval
      dockerfile: Dockerfile
    # image: khanhle04/context-retrieval:latest  # Use this if image is pre-built
    environment:
      QDRANT_URL: "http://qdrant:6333"
      QDRANT_COLLECTION: "mental_health_vi"
      EMBEDDING_SERVICE_URL: "http://deploy-embedding:5000"
      MAX_RESULTS: "20"
      PORT: "5005"
      PYTHONUNBUFFERED: "1"
    ports:
      - "5005:5005"
    depends_on:
      - qdrant
      - deploy-embedding
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "1.0"
        reservations:
          memory: 512M
          cpus: "0.5"

  # FastAPI Embedding Service
  deploy-embedding:
    build:
      context: ./deploy_embedding
      dockerfile: Dockerfile
    # image: khanhle04/deploy-embedding:latest  # Use this if image is pre-built
    environment:
      PORT: "5000"
      PYTHONUNBUFFERED: "1"
      HF_HOME: "/app/.cache/huggingface"
    ports:
      - "5000:5000"
    volumes:
      - model_cache:/app/.cache/huggingface  # Shared model cache
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 3G
          cpus: "2.0"
        reservations:
          memory: 1G
          cpus: "1.0"

volumes:
  qdrant_storage:
  model_cache:  # Shared cache for models
