services:
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
    volumes:
      - qdrant_storage:/qdrant/storage

  # Context Retrieval Service
  context_retrieval_service:
    # build:
    #   context: ./context-retrieval
    #   dockerfile: Dockerfile
    image: khanhle04/context_retrieval_service:2.0  # Use this if image is pre-built
    environment:
      QDRANT_URL: "http://qdrant:6333"
      QDRANT_COLLECTION: "mental_health_vi"
      EMBEDDING_SERVICE_URL: "http://embedding_service:5000"
      MAX_RESULTS: "20"
      PORT: "5005"
      PYTHONUNBUFFERED: "1"
    ports:
      - "5005:5005"
    depends_on:
      - qdrant
      - embedding_service
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "1.0"
        reservations:
          memory: 512M
          cpus: "0.5"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5005/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # FastAPI Embedding Service
  embedding_service:
    # build:
    #   context: ./deploy_embedding
    #   dockerfile: Dockerfile
    image: khanhle04/embedding_service:2.0
    environment:
      PORT: "5000"
      PYTHONUNBUFFERED: "1"
      HF_HOME: "/app/.cache/huggingface"
    ports:
      - "5000:5000"
    volumes:
      - model_cache:/app/.cache/huggingface  # Shared model cache
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 3G
          cpus: "2.0"
        reservations:
          memory: 1G
          cpus: "1.0"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

volumes:
  qdrant_storage:
  model_cache:  # Shared cache for models
