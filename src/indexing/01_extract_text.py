import json
import re
from PyPDF2 import PdfReader
import fitz  # PyMuPDF
from langchain.text_splitter import RecursiveCharacterTextSplitter

def extract_text_with_pypdf2(pdf_path):
    """Extract text from a PDF file using PyPDF2."""
    text = ""
    reader = PdfReader(pdf_path)
    for page in reader.pages:
        text += page.extract_text()
    return text

def extract_text_with_fitz(pdf_path):
    """Extract text from a PDF file using fitz (PyMuPDF)."""
    text = ""
    doc = fitz.open(pdf_path)
    for page_num in range(len(doc)):  # Start from 0 to include all pages
        page = doc.load_page(page_num)
        text += page.get_text("text")
    return text

def preprocess_and_chunk_text(text):
    """
    Preprocess the text and split it into chunks with titles and contexts.
    - Handles structure like: "PH·∫¶N 1", "1.1. N·ªôi dung", "1.2. N·ªôi dung"
    - Titles include both the current part and section.
    - Contexts contain the text under each section.
    """
    # Define regex patterns for identifying parts and sections
    # Pattern for "Ph·∫ßn X" or "PH·∫¶N X" with optional title 
    part_pattern = r"(PH·∫¶N\s+\d+(?:\.|:)?\s*[^\n]*)"
    # Pattern for numbered sections like "1.1.", "1.2.", "2.1.1." etc.
    # Also handles Roman numerals and letters like "I.", "A.", "a)", etc.
    section_pattern = r"(\d+(?:\.\d+)*\.?\s+[A-Z√Ä√Å·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨ƒê√à√â·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞][^\n]*)"
    
    # Split the text into sections based on parts and sections
    sections = re.split(f"({part_pattern}|{section_pattern})", text, flags=re.DOTALL)
    
    # Initialize variables for processing
    current_part = None
    current_section = None
    chunks = []
    buffer = ""
    
    for section in sections:
        # Skip None or empty sections
        if section is None or not section.strip():
            continue
        
        # Check if the section is a part title (PH·∫¶N X)
        part_match = re.match(part_pattern, section.strip())
        if part_match:
            # If there's a previous section, save its content as a chunk
            if current_section and buffer.strip():
                # Create title in format: "Ph·∫ßn X: Title, Section"
                chunk_title = f"{current_part}, {current_section}"
                
                chunk = {
                    "title": chunk_title,
                    "context": buffer.strip()
                }
                chunks.append(chunk)
            
            # Update the current part
            current_part = section.strip()
            current_section = None  # Reset section when a new part starts
            buffer = ""  # Reset buffer for new part
            continue
        
        # Check if the section is a numbered section (1.1., 1.2., etc.)
        section_match = re.match(section_pattern, section.strip())
        if section_match:
            # If there's a previous section, save its content as a chunk
            if current_section and buffer.strip():
                # Create title in format: "Ph·∫ßn X: Title, Section"
                chunk_title = f"{current_part}, {current_section}" if current_part else current_section
                    
                chunk = {
                    "title": chunk_title,
                    "context": buffer.strip()
                }
                chunks.append(chunk)
            
            # Update the current section
            current_section = section.strip()
            buffer = ""  # Reset buffer for new section
            continue
        
        # If it's neither a part nor a section, it's part of the current section's content
        if current_section:
            buffer += " " + section.strip()
    
    # Add the last chunk if there's any remaining content
    if current_section and buffer.strip():
        # Create title in format: "Ph·∫ßn X: Title, Section"
        chunk_title = f"{current_part}, {current_section}" if current_part else current_section
            
        chunk = {
            "title": chunk_title,
            "context": buffer.strip()
        }
        chunks.append(chunk)
    
    return chunks

def split_long_context(title, context, max_length=800):
    """
    Split long context into smaller chunks using RecursiveCharacterTextSplitter.
    Each smaller chunk retains the same title.
    """
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=max_length,
        chunk_overlap=300,  # Overlap to ensure continuity between chunks
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    sub_chunks = splitter.split_text(context)
    return [{"title": title, "context": sub_chunk.strip()} for sub_chunk in sub_chunks]

def process_pdf(pdf_path, output_json, extraction_method="fitz"):
    """Process the PDF and save the output as JSON."""
    print(f"üîÑ Processing PDF: {pdf_path}")
    print(f"üìÅ Output: {output_json}")
    print(f"üîß Method: {extraction_method}")
    print("=" * 60)
    
    # Step 1: Extract text from the PDF using the specified method
    if extraction_method == "pypdf2":
        raw_text = extract_text_with_pypdf2(pdf_path)
    elif extraction_method == "fitz":
        raw_text = extract_text_with_fitz(pdf_path)
    else:
        raise ValueError("Unsupported extraction method. Choose 'pypdf2' or 'fitz'.")
    
    print(f"üìä Total text length: {len(raw_text):,} characters")
    print(f"üìÑ Text preview (first 300 chars):")
    print("-" * 40)
    print(raw_text[:300])
    print("-" * 40)
    
    # Step 2: Preprocess and chunk the text
    print("\nüîç Extracting sections and chunks...")
    chunks = preprocess_and_chunk_text(raw_text)
    
    # Step 3: Split long contexts into smaller chunks
    print(f"üìã Initial chunks found: {len(chunks)}")
    final_chunks = []
    
    for i, chunk in enumerate(chunks):
        title = chunk["title"]
        context = chunk["context"]
        
        print(f"  Chunk {i+1}: '{title}' ({len(context)} chars)")
        
        if len(context) > 800:  # If context is too long, split it
            sub_chunks = split_long_context(title, context)
            final_chunks.extend(sub_chunks)
            print(f"    ‚Üí Split into {len(sub_chunks)} smaller chunks")
        else:
            final_chunks.append(chunk)
    
    print(f"\nüìä Final statistics:")
    print(f"   Total chunks: {len(final_chunks)}")
    print(f"   Average length: {sum(len(c['context']) for c in final_chunks) // len(final_chunks) if final_chunks else 0} chars")
    
    # Show sample chunks
    print(f"\nüìù Sample chunks:")
    for i, chunk in enumerate(final_chunks[:3], 1):
        print(f"   {i}. Title: {chunk['title']}")
        print(f"      Context: {chunk['context'][:80]}...")
    
    # Step 4: Save the chunks to a JSON file
    save_to_json(final_chunks, output_json)
    print(f"\n‚úÖ Saved {len(final_chunks)} chunks to {output_json}")
    
    return final_chunks

def save_to_json(data, output_file):
    """Save the processed data to a JSON file."""
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

def test_patterns():
    """Test the regex patterns with sample text."""
    sample_text = """
    PH·∫¶N 1. T·ªîNG QUAN V·ªÄ T√ÇM L√ù H·ªåC

    1.1. Kh√°i ni·ªám v·ªÅ t√¢m l√Ω h·ªçc
    T√¢m l√Ω h·ªçc l√† khoa h·ªçc nghi√™n c·ª©u v·ªÅ ho·∫°t ƒë·ªông t√¢m l√Ω c·ªßa con ng∆∞·ªùi. ƒê√¢y l√† m·ªôt ng√†nh khoa h·ªçc quan tr·ªçng gi√∫p hi·ªÉu ƒë∆∞·ª£c b·∫£n ch·∫•t c·ªßa con ng∆∞·ªùi.

    1.2. ƒê·ªëi t∆∞·ª£ng nghi√™n c·ª©u c·ªßa t√¢m l√Ω h·ªçc
    ƒê·ªëi t∆∞·ª£ng nghi√™n c·ª©u c·ªßa t√¢m l√Ω h·ªçc bao g·ªìm c√°c hi·ªán t∆∞·ª£ng t√¢m l√Ω nh∆∞ nh·∫≠n th·ª©c, c·∫£m x√∫c, h√†nh vi v√† c√°c qu√° tr√¨nh t√¢m l√Ω kh√°c.

    PH·∫¶N 2. C√ÅC PH∆Ø∆†NG PH√ÅP NGHI√äN C·ª®U

    2.1. Ph∆∞∆°ng ph√°p quan s√°t
    Quan s√°t l√† ph∆∞∆°ng ph√°p nghi√™n c·ª©u c∆° b·∫£n trong t√¢m l√Ω h·ªçc. Ph∆∞∆°ng ph√°p n√†y cho ph√©p nghi√™n c·ª©u vi√™n quan s√°t h√†nh vi t·ª± nhi√™n c·ªßa ƒë·ªëi t∆∞·ª£ng nghi√™n c·ª©u.

    2.2. Ph∆∞∆°ng ph√°p th·ª≠ nghi·ªám
    Th·ª≠ nghi·ªám cho ph√©p nghi√™n c·ª©u m·ªëi quan h·ªá nh√¢n qu·∫£ gi·ªØa c√°c bi·∫øn s·ªë. ƒê√¢y l√† ph∆∞∆°ng ph√°p c√≥ t√≠nh khoa h·ªçc cao v√† ƒë∆∞·ª£c s·ª≠ d·ª•ng r·ªông r√£i.
    """
    
    print("üß™ Testing extraction patterns...")
    print("=" * 60)
    chunks = preprocess_and_chunk_text(sample_text)
    
    print(f"üìä Number of chunks extracted: {len(chunks)}")
    print("=" * 60)
    
    for i, chunk in enumerate(chunks, 1):
        print(f"\nüìù Chunk {i}:")
        print(f"üìå Title: {chunk['title']}")
        print(f"üìÑ Context: {chunk['context'][:100]}...")
        print("-" * 40)
    
    print(f"\n‚úÖ Test completed! Expected format:")
    print(f"   Title: 'PH·∫¶N X: Title, Section'")
    print(f"   Context: 'Content of that section'")
    
    return chunks

# Example usage
if __name__ == "__main__":
    import sys
    from pathlib import Path
    
    # Check if user wants to test patterns
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        test_patterns()
    elif len(sys.argv) > 1 and sys.argv[1] == "all":
        # Process all PDFs in data/raw/
        raw_dir = Path("../../data/raw/")
        pdf_files = list(raw_dir.glob("*.pdf"))
        
        if not pdf_files:
            print("‚ùå No PDF files found in data/raw/")
            sys.exit(1)
        
        print(f"üìö Found {len(pdf_files)} PDF files to process:")
        for pdf in pdf_files:
            print(f"   - {pdf.name}")
        
        print("\n" + "="*80)
        
        for i, pdf_path in enumerate(pdf_files, 1):
            print(f"\nüîÑ Processing {i}/{len(pdf_files)}: {pdf_path.name}")
            output_json = f"../../data/processed/chunks/{pdf_path.stem}.json"
            
            try:
                process_pdf(str(pdf_path), output_json, "fitz")
                print(f"‚úÖ Completed: {pdf_path.name}")
            except Exception as e:
                print(f"‚ùå Failed: {pdf_path.name} - {e}")
            
            if i < len(pdf_files):
                print("\n" + "-"*60)
        
        print(f"\nüéâ Finished processing all {len(pdf_files)} PDF files!")
        
    else:
        # Process single PDF (default behavior)
        pdf_path = "../../data/raw/MOET_SoTay_ThucHanh_CTXH_TrongTruongHoc_vi.pdf"
        output_json = "output.json"
        
        # Choose the extraction method: "pypdf2" or "fitz"
        extraction_method = "fitz"  # Change to "pypdf2" if needed
        
        try:
            process_pdf(pdf_path, output_json, extraction_method)
        except FileNotFoundError:
            print(f"‚ùå PDF file not found: {pdf_path}")
            print("üí° Usage:")
            print("   python 01_extract_text.py       # Process default PDF")
            print("   python 01_extract_text.py test  # Test patterns")
            print("   python 01_extract_text.py all   # Process all PDFs in data/raw/")